!pip install -q transformers datasets accelerate evaluate gradio
from datasets import load_dataset

# Load AG News dataset
dataset = load_dataset("ag_news")

dataset
/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: 
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
README.md: 
 8.07k/? [00:00<00:00, 235kB/s]
data/train-00000-of-00001.parquet: 100%
 18.6M/18.6M [00:00<00:00, 39.0MB/s]
data/test-00000-of-00001.parquet: 100%
 1.23M/1.23M [00:00<00:00, 3.88MB/s]
Generating train split: 100%
 120000/120000 [00:00<00:00, 544265.30 examples/s]
Generating test split: 100%
 7600/7600 [00:00<00:00, 154943.64 examples/s]
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 120000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 7600
    })
})
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
config.json: 100%
 570/570 [00:00<00:00, 48.1kB/s]
tokenizer_config.json: 100%
 48.0/48.0 [00:00<00:00, 4.86kB/s]
vocab.txt: 100%
 232k/232k [00:00<00:00, 3.55MB/s]
tokenizer.json: 100%
 466k/466k [00:00<00:00, 26.7MB/s]
def tokenize_function(example):
    return tokenizer(example["text"], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

tokenized_datasets = tokenized_datasets.remove_columns(["text"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")

tokenized_datasets.set_format("torch")
Map: 100%
 120000/120000 [01:04<00:00, 2004.10 examples/s]
Map: 100%
 7600/7600 [00:03<00:00, 2081.09 examples/s]
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=4
)
model.safetensors: 100%
 440M/440M [00:04<00:00, 87.8MB/s]
Loading weights: 100%
 199/199 [00:00<00:00, 554.31it/s, Materializing param=bert.pooler.dense.weight]
BertForSequenceClassification LOAD REPORT from: bert-base-uncased
Key                                        | Status     | 
-------------------------------------------+------------+-
cls.predictions.bias                       | UNEXPECTED | 
cls.predictions.transform.dense.weight     | UNEXPECTED | 
cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | 
cls.seq_relationship.weight                | UNEXPECTED | 
cls.predictions.transform.LayerNorm.weight | UNEXPECTED | 
cls.seq_relationship.bias                  | UNEXPECTED | 
cls.predictions.transform.dense.bias       | UNEXPECTED | 
classifier.bias                            | MISSING    | 
classifier.weight                          | MISSING    | 

Notes:
- UNEXPECTED	:can be ignored when loading from different task/architecture; not ok if you expect identical arch.
- MISSING	:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.
import evaluate
import numpy as np

accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    
    return {
        "accuracy": accuracy.compute(predictions=predictions, references=labels)["accuracy"],
        "f1": f1.compute(predictions=predictions, references=labels, average="weighted")["f1"]
    }
